import asyncio
import json
import time
import google.generativeai as genai
from typing import List, Dict
from dotenv import load_dotenv
import os

load_dotenv()


genai.configure(api_key=os.getenv("GEMINI_API_KEY"))

model = genai.GenerativeModel(
    model_name="gemini-1.5-flash",
    generation_config={
        "temperature": 0.3,
        "top_p": 1,
        "top_k": 40,
        "max_output_tokens": 999999
    }
)


def parse_chunk(index: int, data: Dict[str, str]) -> List[Dict]:
    text = data["solar_panels"]
    prompt = f"""
Ğ”Ñ–ÑĞ¹ ÑĞº Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑ–Ğ¹Ğ½Ğ¸Ğ¹ Ğ¿Ğ°Ñ€ÑĞµÑ€ Ñ– ÑĞ¿ĞµÑ†Ñ–Ğ°Ğ»Ñ–ÑÑ‚ Ğ· Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶Ñƒ ÑĞ¾Ğ½ÑÑ‡Ğ½Ğ¸Ñ… Ğ¿Ğ°Ğ½ĞµĞ»ĞµĞ¹.

Ğ— Ñ†ÑŒĞ¾Ğ³Ğ¾ HTML-Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ñƒ Ğ¿Ğ¾Ğ²Ğ½Ñ–ÑÑ‚Ñ Ğ²Ğ¸Ñ‚ÑĞ³Ğ½Ğ¸ Ğ´Ğ°Ğ½Ñ– Ğ¿Ñ€Ğ¾ ÑĞ¾Ğ½ÑÑ‡Ğ½Ñ– Ğ¿Ğ°Ğ½ĞµĞ»Ñ– Ñ‚Ğ° Ğ¿ĞµÑ€ĞµÑ‚Ğ²Ğ¾Ñ€Ğ¸ Ñ—Ñ… Ñƒ Ğ¼Ğ°ÑĞ¸Ğ² JSON Ğ¾Ğ±'Ñ”ĞºÑ‚Ñ–Ğ² Ñ‚Ğ°ĞºĞ¾Ğ³Ğ¾ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ñƒ:
{{
  "brand": brand,
  "power": power,
  "full_name": full_name,
  "price": price,
  "panel_type": panel_type,
  "cell_type": cell_type,
  "thickness": thickness,
  "panel_color": panel_color,
  "frame_color": frame_color
}}

ğŸ“Œ Ğ”ĞµÑ‚Ğ°Ğ»Ñ– Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ñƒ:
- 'brand': Ğ¿Ğ¾Ğ²ĞµÑ€Ğ½Ğ¸ Ğ½Ğ°Ğ·Ğ²Ñƒ Ğ±Ñ€ĞµĞ½Ğ´Ñƒ Ğ¾Ğ´Ğ½Ğ¸Ğ¼ ÑĞ»Ğ¾Ğ²Ğ¾Ğ¼
- `price`: Ñ†Ñ–Ğ½Ğ° Ğ² Ğ´Ğ¾Ğ»Ğ°Ñ€Ğ°Ñ… Ğ¡Ğ¨Ğ (ÑĞºÑ‰Ğ¾ Ğ½ĞµĞ¼Ğ¾Ğ¶Ğ»Ğ¸Ğ²Ğ¾ Ğ·Ğ½Ğ°Ğ¹Ñ‚Ğ¸ Ñ‚Ğ¾ 0)
- `full_name`: Ğ¿Ğ¾Ğ²Ğ½Ğ° Ğ½Ğ°Ğ·Ğ²Ğ° ÑĞ¾Ğ½ÑÑ‡Ğ½Ğ¾Ñ— Ğ¿Ğ°Ğ½ĞµĞ»Ñ–
- `power`: Ğ¿Ğ¾Ñ‚ÑƒĞ¶Ğ½Ñ–ÑÑ‚ÑŒ Ñƒ Ğ’Ğ°Ñ‚Ğ°Ñ… (Ğ’Ñ‚/W) (ÑĞºÑ‰Ğ¾ Ğ½ĞµĞ¼Ğ°Ñ” Ñ‚Ğ¾ 0)
- `panel_type`: Ñ‚Ğ¸Ğ¿ Ğ¿Ğ°Ğ½ĞµĞ»Ñ– - "Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ñ" Ğ°Ğ±Ğ¾ "Ğ´Ğ²Ğ¾ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ñ" (Ğ·Ğ° Ğ·Ğ°Ğ¼Ğ¾Ğ²Ñ‡ÑƒĞ²Ğ°Ğ½Ğ½ÑĞ¼ "Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ñ")
- `cell_type`: Ñ‚Ğ¸Ğ¿ ĞºĞ¾Ğ¼Ñ–Ñ€Ğ¾Ğº - "n-type" Ğ°Ğ±Ğ¾ "p-type" (Ğ·Ğ° Ğ·Ğ°Ğ¼Ğ¾Ğ²Ñ‡ÑƒĞ²Ğ°Ğ½Ğ½ÑĞ¼ "n-type")
- `thickness`: Ñ‚Ğ¾Ğ²Ñ‰Ğ¸Ğ½Ğ° Ğ² Ğ¼Ñ–Ğ»Ñ–Ğ¼ĞµÑ‚Ñ€Ğ°Ñ… (Ğ¼Ğ¼) (Ğ·Ğ° Ğ·Ğ°Ğ¼Ğ¾Ğ²Ñ‡ÑƒĞ²Ğ°Ğ½Ğ½ÑĞ¼ 30)
- `panel_color`: ĞºĞ¾Ğ»Ñ–Ñ€ Ğ¿Ğ°Ğ½ĞµĞ»Ñ– (Ğ·Ğ° Ğ·Ğ°Ğ¼Ğ¾Ğ²Ñ‡ÑƒĞ²Ğ°Ğ½Ğ½ÑĞ¼ "Default")
- `frame_color`: ĞºĞ¾Ğ»Ñ–Ñ€ Ñ€Ğ°Ğ¼ĞºĞ¸ (Ğ·Ğ° Ğ·Ğ°Ğ¼Ğ¾Ğ²Ñ‡ÑƒĞ²Ğ°Ğ½Ğ½ÑĞ¼ "Silver")

ĞÑÑŒ HTML-Ğ´Ğ°Ğ½Ñ–:
{text}

â—ï¸ĞŸĞ¾Ğ²ĞµÑ€Ğ½Ğ¸ Ğ»Ğ¸ÑˆĞµ Ñ‡Ğ¸ÑÑ‚Ğ¸Ğ¹ JSON Ñƒ Ğ²Ñ–Ğ´Ğ¿Ğ¾Ğ²Ñ–Ğ´ÑŒ. Ğ‘ĞµĞ· Ğ·Ğ°Ğ¹Ğ²Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ.
"""

    try:
        response = model.generate_content(prompt)
        response_text = response.text.strip()

        # ĞÑ‡Ğ¸Ñ‰ĞµĞ½Ğ½Ñ
        if response_text.startswith("```json"):
            response_text = response_text.replace("```json", "", 1)
        if response_text.endswith("```"):
            response_text = response_text.rsplit("```", 1)[0]

        parsed = json.loads(response_text)
        print(f"âœ… ĞĞ±Ñ€Ğ¾Ğ±Ğ»ĞµĞ½Ğ¾ Ğ±Ğ»Ğ¾Ğº {index}: Ğ·Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ {len(parsed)} ÑĞ¾Ğ½ÑÑ‡Ğ½Ğ¸Ñ… Ğ¿Ğ°Ğ½ĞµĞ»ĞµĞ¹")
        return parsed

    except Exception as e:
        print(f"âŒ ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° Ğ½Ğ° Ğ±Ğ»Ğ¾Ñ†Ñ– {index}: {e}")
        return []


async def ai_parser(all_data: List[Dict[str, str]]) -> List[Dict]:
    parsed_results = []
    min_request_time = 10
    for i, data in enumerate(all_data):
        start_time = time.time()
        result = parse_chunk(i, data)
        parsed_results.extend(result)

        elapsed_time = time.time() - start_time
        if elapsed_time < min_request_time and i < len(all_data) - 1:  # Ğ½Ğµ Ñ‡ĞµĞºĞ°Ñ”Ğ¼Ğ¾ Ğ¿Ñ–ÑĞ»Ñ Ğ¾ÑÑ‚Ğ°Ğ½Ğ½ÑŒĞ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ğ¸Ñ‚Ñƒ
            wait_time = min_request_time - elapsed_time
            print(f"â³ Ğ—Ğ°Ğ¿Ğ¸Ñ‚ Ğ²Ğ¸ĞºĞ¾Ğ½Ğ°Ğ½Ğ¾ Ğ·Ğ° {elapsed_time:.1f} ÑĞµĞº. ĞÑ‡Ñ–ĞºÑƒĞ²Ğ°Ğ½Ğ½Ñ {wait_time:.1f} ÑĞµĞºÑƒĞ½Ğ´ Ğ¿ĞµÑ€ĞµĞ´ Ğ½Ğ°ÑÑ‚ÑƒĞ¿Ğ½Ğ¸Ğ¼ Ğ·Ğ°Ğ¿Ğ¸Ñ‚Ğ¾Ğ¼...")
            time.sleep(wait_time)


    return parsed_results




